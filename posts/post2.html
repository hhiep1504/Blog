<!DOCTYPE html>\n<html lang="vi">\n<head>\n    <meta charset="UTF-8">\n    <meta name="viewport" content="width=device-width, initial-scale=1.0">\n    <title>STOCK_NOTEBOOK_DOCUMENTATION</title>\n    <link href="https://fonts.googleapis.com/css2?family=Libre+Baskerville:ital,wght@0,400;0,700;1,400&family=Playfair+Display:wght@400;500;700&family=Fira+Code&display=swap" rel="stylesheet">\n    <style>\n        :root { --bg: #fffce8; --text: #2d2d2d; --heading: #483c32; --accent: #268bd2; }\n        * { margin: 0; padding: 0; box-sizing: border-box; }\n        body { font-family: "Libre Baskerville", serif; max-width: 760px; margin: 0 auto; padding: 2rem; background: var(--bg); color: var(--text); line-height: 1.8; }\n        h1 { font-family: "Playfair Display", serif; font-size: 2rem; color: var(--heading); text-align: center; margin: 2rem 0 1rem; }\n        h2 { font-family: "Playfair Display", serif; font-size: 1.5rem; color: var(--heading); text-align: center; margin: 3rem 0 2rem; }\n        h3 { font-family: "Playfair Display", serif; font-size: 1.2rem; color: var(--heading); margin: 2rem 0 1rem; }\n        p { margin: 1.5rem 0; text-align: justify; }\n        pre { background: #2d2d2d; padding: 1.5rem; border-radius: 4px; overflow-x: auto; margin: 2rem 0; }\n        code { font-family: "Fira Code", monospace; color: #f8f8f2; }\n        :not(pre) > code { background: #ebe5d8; color: var(--accent); padding: 0.2rem 0.5rem; border-radius: 3px; }\n        blockquote { border-left: 3px solid #927e5a; background: #fffef5; padding: 1rem 1.5rem; margin: 2rem 0; font-style: italic; }\n        ul, ol { padding-left: 2rem; margin: 1.5rem 0; }\n        li { margin: 0.5rem 0; }\n        a { color: var(--accent); text-decoration: none; }\n        .meta { text-align: center; color: #927e5a; font-style: italic; margin-bottom: 2rem; }\n        .back { display: inline-block; margin-bottom: 2rem; color: var(--accent); }\n    </style>\n</head>\n<body>\n    <a href="../index.html" class="back">â† Vá» trang chá»§</a>\n    <h1>STOCK_NOTEBOOK_DOCUMENTATION</h1>\n    <div class="meta">20 thÃ¡ng 1, 2026 â€¢ 5 phÃºt Ä‘á»c</div>\n    <article>\n<h2>Tá»•ng Quan</h2>
<h3>BÃ i toÃ¡n lÃ  gÃ¬?</h3>
<ul>
<li><strong>Input:</strong> GiÃ¡ cá»• phiáº¿u hÃ ng ngÃ y tá»« 2019-2024 + cÃ¡c Ä‘áº·c tÃ­nh thá»‹ trÆ°á»ng</li>
<li><strong>Output:</strong> Dá»± bÃ¡o min/max return cho quÃ½ tiáº¿p theo</li>
<li><strong>Dá»¯ liá»‡u:</strong> 26 mÃ£ cá»• phiáº¿u Viá»‡t Nam (VNM, HPG, VHM, BID, v.v.)</li>
<li><strong>Má»¥c tiÃªu:</strong> Dá»± bÃ¡o chÃ­nh xÃ¡c khoáº£ng dao Ä‘á»™ng giÃ¡ Ä‘á»ƒ há»— trá»£ quyáº¿t Ä‘á»‹nh Ä‘áº§u tÆ°</li>
</ul>
<h3>VÃ¬ sao dÃ¹ng Graph Neural Network?</h3>
<ul>
<li>CÃ¡c cá»• phiáº¿u <strong>khÃ´ng Ä‘á»™c láº­p</strong>, chÃºng liÃªn káº¿t qua:<ul>
<li><strong>NgÃ nh (sector):</strong> CÃ¡c cá»• phiáº¿u cÃ¹ng ngÃ nh (housing, industrial) thÆ°á»ng di chuyá»ƒn cÃ¹ng nhau</li>
<li><strong>TÆ°Æ¡ng quan (correlation):</strong> Náº¿u VNM tÄƒng thÃ¬ HPG cÃ³ xu hÆ°á»›ng tÄƒng</li>
<li><strong>Luáº­t káº¿t há»£p (association rules):</strong> Náº¿u BID â†‘ thÃ¬ VHM â†‘ vá»›i xÃ¡c suáº¥t cao</li>
</ul>
</li>
<li><strong>GAT (Graph Attention Network):</strong> Há»c trá»ng sá»‘ tá»± Ä‘á»™ng cho tá»«ng liÃªn káº¿t</li>
<li><strong>LSTM:</strong> Há»c thÃ nh phá»‘ hÃ³a dá»± bÃ¡o qua thá»i gian</li>
</ul>
<h3>Äá»c File Target (Quarterly Min/Max)</h3>
<pre><code class="language-python">df_target_raw = pd.read_csv(&#39;/content/min_max_return.csv&#39;, header=1)

def clean_float(x):
    try:
        if isinstance(x, str):
            x = x.replace(&#39;,&#39;, &#39;&#39;).replace(&#39;#DIV/0!&#39;, &#39;&#39;).strip()
            if x == &#39;&#39; or x == &#39;-&#39;: return 0.0
        return float(x)
    except:
        return 0.0

df_target_raw = df_target_raw.map(clean_float)
stock_codes = df_target_raw.columns[2:28].tolist()  # Láº¥y 26 mÃ£ cá»• phiáº¿u
</code></pre>
<p><strong>Äiá»u gÃ¬ Ä‘ang xáº£y ra:</strong></p>
<ol>
<li><strong>Äá»c file CSV</strong> chá»©a min/max return theo quÃ½ cho 26 mÃ£<ul>
<li>Cáº¥u trÃºc: cá»™t 2-27 = min return, cá»™t 28-53 = max return</li>
</ul>
</li>
<li><strong>HÃ m <code>clean_float</code></strong> xá»­ lÃ½ dá»¯ liá»‡u báº©n:<ul>
<li>Loáº¡i bá» dáº¥u pháº©y (vÃ­ dá»¥: &quot;1,000&quot; â†’ &quot;1000&quot;)</li>
<li>XÃ³a lá»—i Excel (#DIV/0!)</li>
<li>Chuyá»ƒn string â†’ float</li>
</ul>
</li>
<li><strong>Ãp dá»¥ng cho toÃ n bá»™ dataframe</strong> vá»›i <code>.map()</code></li>
<li><strong>TrÃ­ch xuáº¥t 26 mÃ£</strong> tá»« tÃªn cá»™t</li>
</ol>
<p><strong>Output:</strong></p>
<pre><code>âœ… ÄÃ£ tÃ¬m tháº¥y 26 mÃ£ cá»• phiáº¿u.
stock_codes = [&#39;VNM&#39;, &#39;HPG&#39;, &#39;VHM&#39;, &#39;BID&#39;, &#39;GAS&#39;, ...]
</code></pre>
<hr>
<h3>Äá»c File Daily Prices</h3>
<p><strong>Chi tiáº¿t tá»«ng bÆ°á»›c:</strong></p>
<table>
<thead>
<tr>
<th>BÆ°á»›c</th>
<th>HÃ nh Ä‘á»™ng</th>
<th>Káº¿t quáº£</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>Äá»c CSV giÃ¡ Ä‘Ã³ng cá»­a hÃ ng ngÃ y</td>
<td>DataFrame 6 nÄƒm Ã— 26 cá»• phiáº¿u</td>
</tr>
<tr>
<td>2</td>
<td>Chuyá»ƒn cá»™t &#39;Date&#39; â†’ datetime</td>
<td><code>2024-12-15</code> thay vÃ¬ string</td>
</tr>
<tr>
<td>3</td>
<td>TÃ¡ch Year, Quarter</td>
<td>ThÃªm 2 cá»™t má»›i</td>
</tr>
<tr>
<td>4</td>
<td>LÃ m sáº¡ch giÃ¡ (xÃ³a dáº¥u pháº©y)</td>
<td><code>&quot;1,500&quot;</code> â†’ <code>1500.0</code></td>
</tr>
<tr>
<td>5</td>
<td><strong>Forward fill + backward fill</strong></td>
<td>Náº¿u ngÃ y lá»… missing â†’ láº¥y ngÃ y trÆ°á»›c, náº¿u Ä‘áº§u missing â†’ láº¥y ngÃ y sau</td>
</tr>
<tr>
<td>6</td>
<td>GhÃ©p láº¡i</td>
<td>Dataframe: Date + Year + Quarter + 26 cá»™t giÃ¡</td>
</tr>
</tbody></table>
<p><strong>Output structure:</strong></p>
<pre><code>   Year  Quarter    VNM      HPG      VHM    ...
0  2019        1   100.0    450.0   1200.0   ...
1  2019        1   102.0    451.0   1205.0   ...
   ...
</code></pre>
<hr>
<h3>Cell 4: Feature Engineering</h3>
<p>ÄÃ¢y lÃ  <strong>pháº§n Cá»°C Ká»² QUAN TRá»ŒNG</strong> Ä‘á»ƒ mÃ´ hÃ¬nh há»c tá»‘t.</p>
<pre><code class="language-python"># TÃ­nh Standard Deviation theo quÃ½
input_std = df_daily_merged.groupby([&#39;Year&#39;, &#39;Quarter&#39;]).std()[stock_codes]

# TÃ­nh Mean (giÃ¡ trung bÃ¬nh) theo quÃ½
input_mean = df_daily_merged.groupby([&#39;Year&#39;, &#39;Quarter&#39;]).mean()[stock_codes]

# TÃ­nh return log theo quÃ½
def calc_quarter_log_return(x):
    return np.log(x.iloc[-1] / x.iloc[0]) if x.iloc[0] &gt; 0 else 0

input_ret = df_daily_merged.groupby([&#39;Year&#39;, &#39;Quarter&#39;]).agg(calc_quarter_log_return)[stock_codes]

# TÃ­nh skewness (Ä‘á»™ lá»‡ch cá»§a phÃ¢n phá»‘i return)
daily_returns = df_daily_merged[...].pct_change().dropna()
feat_4 = daily_returns.skew().values  # Skewness
</code></pre>
<p><strong>4 Features mÃ´ hÃ¬nh sáº½ nháº­n:</strong></p>
<table>
<thead>
<tr>
<th>Feature</th>
<th>TÃ­nh toÃ¡n</th>
<th>Ã nghÄ©a</th>
</tr>
</thead>
<tbody><tr>
<td><strong>feat_1</strong></td>
<td><code>log(std)</code></td>
<td><strong>Biáº¿n Ä‘á»™ng</strong> cá»§a giÃ¡ trong quÃ½. Cao = quÃ½ nhiá»u biáº¿n Ä‘á»™ng, tháº¥p = quÃ½ á»•n Ä‘á»‹nh</td>
</tr>
<tr>
<td><strong>feat_2</strong></td>
<td><code>log(mean_price)</code></td>
<td><strong>Má»©c giÃ¡</strong> trung bÃ¬nh. GiÃºp mÃ´ hÃ¬nh hiá»ƒu tá»· lá»‡</td>
</tr>
<tr>
<td><strong>feat_3</strong></td>
<td><code>log(price_end/price_start)</code></td>
<td><strong>Return quÃ½</strong> = % tÄƒng/giáº£m trong quÃ½</td>
</tr>
<tr>
<td><strong>feat_4</strong></td>
<td><code>skewness(daily_returns)</code></td>
<td><strong>Tail behavior</strong> = Ä‘á»™ lá»‡ch cá»§a phÃ¢n phá»‘i. DÆ°Æ¡ng = nhiá»u ngÃ y tÄƒng máº¡nh, Ã‚m = nhiá»u ngÃ y giáº£m máº¡nh</td>
</tr>
</tbody></table>
<p><strong>VÃ­ dá»¥ thá»±c táº¿:</strong></p>
<pre><code>QuÃ½ 1/2024 (VNM):
- std = 1.5 â†’ log(1.5) â‰ˆ 0.41  (quÃ½ nÃ y hÆ¡i biáº¿n Ä‘á»™ng)
- mean = 100 â†’ log(100) â‰ˆ 4.61 (má»©c giÃ¡ trung bÃ¬nh)
- return = 0.05 â†’ log(1.05) â‰ˆ 0.049 (tÄƒng 5% trong quÃ½)
- skewness = 0.3 â†’ hÆ¡i lá»‡ch dÆ°Æ¡ng (xu hÆ°á»›ng tÄƒng)
</code></pre>
<hr>
<h3>Cell 5: Táº¡o Training &amp; Target Tensors</h3>
<pre><code class="language-python">sorted_indices = sorted(input_std.index)  # [(2019,4), (2020,1), (2020,2), ...]

x_all_list = []
y_all_list = []
valid_indices_map = []

for y, q in sorted_indices:
    # Láº¥y dÃ²ng target á»©ng vá»›i quÃ½ nÃ y
    row = df_target_raw[(df_target_raw[&#39;Nam&#39;] == y) &amp; (df_target_raw[&#39;Quy&#39;] == q)]

    if not row.empty and (y, q) in input_std.index:
        # ===== INPUT (X) =====
        feat_1 = np.log1p(input_std.loc[(y, q)].values)      # [26] values
        feat_2 = np.log1p(input_mean.loc[(y, q)].values)     # [26]
        feat_3 = input_ret.loc[(y, q)].values                # [26]
        feat_4 = daily_returns.skew().values                 # [26]
        
        x_val = np.stack([feat_1, feat_2, feat_3, feat_4], axis=1)  # Shape: (26, 4)
        x_all_list.append(x_val)
        
        # ===== TARGET (Y) =====
        y_min = df_min.loc[row.index[0]].values    # Min return quÃ½ tiáº¿p theo, [26]
        y_max = df_max.loc[row.index[0]].values    # Max return quÃ½ tiáº¿p theo, [26]
        y_val = np.stack([y_min, y_max], axis=1)   # Shape: (26, 2)
        y_all_list.append(y_val)
        
        valid_indices_map.append((y, q))

# Chuyá»ƒn list â†’ numpy arrays â†’ tensors
X_temp = np.array(x_all_list)                          # Shape: (num_quarters, 26, 4)
Y_full = torch.tensor(np.array(y_all_list), dtype=torch.float)  # Shape: (num_quarters, 26, 2)
</code></pre>
<p><strong>Cáº¥u trÃºc dá»¯ liá»‡u:</strong></p>
<pre><code>X_temp[0] = [[feat_1[0], feat_2[0], feat_3[0], feat_4[0]],   # QuÃ½ 1 - Cá»• phiáº¿u 1
             [feat_1[1], feat_2[1], feat_3[1], feat_4[1]],   # QuÃ½ 1 - Cá»• phiáº¿u 2
             ...
             [feat_1[25], feat_2[25], feat_3[25], feat_4[25]]]  # QuÃ½ 1 - Cá»• phiáº¿u 26

Y_full[0] = [[y_min[0], y_max[0]],      # QuÃ½ tiáº¿p (dá»± bÃ¡o) - Cá»• phiáº¿u 1
             [y_min[1], y_max[1]],      # QuÃ½ tiáº¿p - Cá»• phiáº¿u 2
             ...
             [y_min[25], y_max[25]]]    # QuÃ½ tiáº¿p - Cá»• phiáº¿u 26
</code></pre>
<hr>
<h3>Cell 6: Chuáº©n HÃ³a Dá»¯ Liá»‡u (Crucial!)</h3>
<pre><code class="language-python">split_idx = -4  # Láº¥y 4 quÃ½ cuá»‘i lÃ m test

X_train_raw = X_temp[:split_idx]        # ToÃ n bá»™ quÃ½ trá»« 4 quÃ½ cuá»‘i
X_test_raw = X_temp[split_idx:]         # 4 quÃ½ cuá»‘i

# ===== BÆ¯á»šC Cá»°C Ká»² QUAN TRá»ŒNG: Fit scaler CHá»ˆ trÃªn TRAIN =====
num_samples_train, num_nodes, num_feats = X_train_raw.shape
X_train_reshaped = X_train_raw.reshape(-1, num_feats)  # Pháº³ng thÃ nh (samples*26, 4)

scaler = StandardScaler()
scaler.fit(X_train_reshaped)                          # TÃ­nh mean, std tá»« TRAIN ONLY
X_train_scaled = scaler.transform(X_train_reshaped)   # Ãp dá»¥ng vÃ o TRAIN

# ===== CÃ“ QUAN TRá»ŒNG: Transform TEST báº±ng scaler tá»« TRAIN =====
X_test_scaled = scaler.transform(X_test_raw.reshape(-1, num_feats))

# GhÃ©p láº¡i
X_full = torch.tensor(np.concatenate([X_train_scaled, X_test_scaled], axis=0), 
                      dtype=torch.float)

print(f&quot;Shape: {X_full.shape}&quot;)  # torch.Size([num_quarters, 26, 4])
</code></pre>
<p><strong>Táº¡i sao viá»‡c nÃ y quan trá»ng?</strong></p>
<p>âŒ <strong>Sai:</strong> Fit scaler trÃªn toÃ n bá»™ dá»¯ liá»‡u (train + test)</p>
<ul>
<li>LÃ m data leakage: scaler biáº¿t vá» future data</li>
<li>MÃ´ hÃ¬nh sáº½ biáº¿t test data khÃ´ng thá»±c táº¿</li>
</ul>
<p>âœ… <strong>ÄÃºng:</strong> Fit scaler CHá»ˆ trÃªn train</p>
<ul>
<li>Scaler chá»‰ dÃ¹ng mean/std tá»« train</li>
<li>Apply vÃ o test báº±ng scaler nÃ y</li>
<li>MÃ´ hÃ¬nh khÃ´ng biáº¿t test data</li>
</ul>
<hr>
<h2>XÃ¢y Dá»±ng Graph (Mathematical Graph Construction)</h2>
<p>mÃ´ hÃ¬nh hÃ³a thá»‹ trÆ°á»ng dÆ°á»›i dáº¡ng má»™t Ä‘á»“ thá»‹ $G = (V, E)$.</p>
<ul>
<li><strong>Nodes ($V$):</strong> Táº­p há»£p 26 mÃ£ cá»• phiáº¿u.</li>
<li><strong>Edges ($E$):</strong> Táº­p há»£p cÃ¡c má»‘i quan há»‡ phá»¥ thuá»™c giá»¯a cÃ¡c cá»• phiáº¿u.</li>
<li><strong>Má»¥c tiÃªu:</strong> XÃ¢y dá»±ng Ma tráº­n ká» (Adjacency Matrix) $A \in {0, 1}^{26 \times 26}$ Ä‘á»ƒ Ä‘á»‹nh hÆ°á»›ng luá»“ng thÃ´ng tin trong máº¡ng GAT.</li>
</ul>
<p>káº¿t há»£p hai loáº¡i cáº¡nh: <strong>Static</strong> (Cá»‘ Ä‘á»‹nh) vÃ  <strong>Dynamic</strong> (Thay Ä‘á»•i theo thá»i gian).</p>
<hr>
<h3>Static Edges - Sector-based Topology</h3>
<p><strong>CÃ´ng thá»©c toÃ¡n há»c:</strong>
Gá»i $S(i)$ lÃ  phÃ¢n loáº¡i ngÃ nh (Sector) cá»§a cá»• phiáº¿u $i$. Táº­p cáº¡nh tÄ©nh $E_{static}$ Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a:
$$ E_{static} = { (i, j) \mid S(i) = S(j), i \neq j } $$</p>
<pre><code class="language-python"># Code implementation logic
sector_map = {c: sector_raw[c] for c in stock_codes if c in sector_raw}
# ... (táº¡o cáº¡nh náº¿u cÃ¹ng sector)
</code></pre>
<h3>Dynamic Edges - Data-Driven Dependencies</h3>
<p>Má»‘i quan há»‡ trÃªn thá»‹ trÆ°á»ng tÃ i chÃ­nh khÃ´ng cá»‘ Ä‘á»‹nh -&gt; xÃ¢y dá»±ng $E_{dynamic}^{(t)}$ cho tá»«ng cá»­a sá»• thá»i gian $t$ dá»±a trÃªn dá»¯ liá»‡u giao dá»‹ch thá»±c táº¿</p>
<h4>2. K-Nearest Neighbors (KNN) - Pearson Correlation</h4>
<p>PhÆ°Æ¡ng phÃ¡p nÃ y Ä‘o lÆ°á»ng sá»± Ä‘á»“ng biáº¿n Ä‘á»™ng tuyáº¿n tÃ­nh (linear co-movement).</p>
<ul>
<li><strong>Pearson Correlation:</strong> TÃ­nh ma tráº­n tÆ°Æ¡ng quan $C$ cho cá»­a sá»• thá»i gian hiá»‡n táº¡i:
$$ C_{ij} = \frac{\text{cov}(R_i, R_j)}{\sigma_{R_i} \sigma_{R_j}} $$</li>
<li><strong>Edge Selection:</strong> Cáº¡nh $(i, j)$ Ä‘Æ°á»£c táº¡o náº¿u thá»a mÃ£n 2 Ä‘iá»u kiá»‡n:<ol>
<li>$|C_{ij}| &gt; 0.5$ (TÆ°Æ¡ng quan Ä‘á»§ máº¡nh)</li>
<li>$j \in \text{Top-}k(i)$ (Thuá»™c nhÃ³m $k$ lÃ¡ng giá»ng gáº§n nháº¥t cá»§a $i$)</li>
</ol>
</li>
</ul>
<p><strong>Tá»•ng há»£p Graph:</strong>
Táº­p cáº¡nh cuá»‘i cÃ¹ng cho má»—i máº«u dá»¯ liá»‡u lÃ  há»£p cá»§a cÃ¡c táº­p cáº¡nh trÃªn:
$$ E_{final}^{(t)} = E_{static} \cup E_{KNN}^{(t)} $$</p>
<p>Äiá»u nÃ y Ä‘áº£m báº£o Ä‘á»“ thá»‹ vá»«a cÃ³ tÃ­nh á»•n Ä‘á»‹nh cáº¥u trÃºc (Static), vá»«a thÃ­ch á»©ng vá»›i biáº¿n Ä‘á»™ng thá»‹ trÆ°á»ng (Dynamic).</p>
<hr>
<h3>Sequence Construction - Tensor Formation</h3>
<p>QuÃ¡ trÃ¬nh nÃ y chuyá»ƒn Ä‘á»•i dá»¯ liá»‡u báº£ng thÃ nh cÃ¡c Tensor 3 chiá»u Ä‘á»ƒ Ä‘Æ°a vÃ o mÃ´ hÃ¬nh Deep Learning.</p>
<ul>
<li><strong>Sliding Window:</strong> $W = 2$ (QuÃ½).</li>
<li><strong>Input Tensor ($X$):</strong> Vá»›i má»—i bÆ°á»›c thá»i gian $t$, ta láº¥y cá»­a sá»• $[t, t+1]$.<ul>
<li>Shape: $(Batch, Sequence, Nodes, Features)$</li>
</ul>
</li>
<li><strong>Graph Structure ($A$):</strong> Táº¡i má»—i bÆ°á»›c $t$, ta tÃ­nh láº¡i $E_{final}^{(t)}$ dá»±a trÃªn dá»¯ liá»‡u cá»§a cá»­a sá»• Ä‘Ã³.</li>
</ul>
<p>Káº¿t quáº£ lÃ  má»™t danh sÃ¡ch cÃ¡c tuple $(X_{seq}, Y_{target}, A_{adj})$, trong Ä‘Ã³ $A_{adj}$ thay Ä‘á»•i linh hoáº¡t theo tá»«ng máº«u dá»¯ liá»‡u, cho phÃ©p GAT há»c Ä‘Æ°á»£c cáº¥u trÃºc Ä‘á»™ng cá»§a thá»‹ trÆ°á»ng.</p>
<hr>
<h2>3ï¸âƒ£ Model Architecture (GAT-LSTM)</h2>
<h3>Äá»‹nh nghÄ©a Model</h3>
<pre><code class="language-python">class StockGAT_LSTM(torch.nn.Module):
    def __init__(self, num_nodes, in_features=4, gnn_hidden=128, lstm_hidden=256, heads=2):
        super(StockGAT_LSTM, self).__init__()
        
        # ===== PHáº¦N 1: GAT (Graph Attenion) =====
        # Input: [num_nodes=26, in_features=4]
        # Output: [num_nodes=26, gnn_hidden*heads=256]
        self.conv1 = GATConv(in_features, gnn_hidden, heads=heads, dropout=0.6)
        
        # ===== PHáº¦N 2: LSTM =====
        # Input: [num_nodes=26, seq_len=2, features=256]
        # Output: hidden state cuá»‘i [num_nodes=26, lstm_hidden=256]
        self.lstm = torch.nn.LSTM(input_size=gnn_hidden*heads, 
                                   hidden_size=lstm_hidden, 
                                   batch_first=True, 
                                   dropout=0.2)
        
        # ===== PHáº¦N 3: Linear (Regression Head) =====
        # Input: [num_nodes=26, lstm_hidden=256]
        # Output: [num_nodes=26, 2] (min, max)
        self.linear = torch.nn.Linear(lstm_hidden, 2)

    def forward(self, x_seq, edge_index):
        &quot;&quot;&quot;
        Args:
            x_seq:      [seq_len=2, num_nodes=26, in_features=4]
            edge_index: [2, num_edges]
        
        Returns:
            out: [num_nodes=26, 2] (predicted min, max)
        &quot;&quot;&quot;
        seq_len = x_seq.size(0)  # 2
        gnn_out_list = []
        
        # ===== BÆ°á»›c 1: ÄÆ°a tá»«ng quÃ½ qua GAT =====
        for t in range(seq_len):  # t = 0, 1
            xt = x_seq[t]                      # Shape: [26, 4]
            
            # GAT: há»c trá»ng sá»‘ cáº¡nh, aggregate hÃ ng xÃ³m
            out = self.conv1(xt, edge_index)   # [26, 4] â†’ [26, 256]
            out = F.elu(out)                   # Activation
            
            gnn_out_list.append(out)
        
        # ===== BÆ°á»›c 2: Stack Ä‘á»ƒ táº¡o chuá»—i =====
        spatial_seq = torch.stack(gnn_out_list, dim=0)  # [2, 26, 256]
        
        # ===== BÆ°á»›c 3: ÄÆ°a qua LSTM =====
        # LSTM expects: [batch_size, seq_len, features]
        lstm_input = spatial_seq.permute(1, 0, 2)      # [26, 2, 256]
        lstm_out, (h_n, c_n) = self.lstm(lstm_input)   # h_n: [1, 26, 256]
        
        # ===== BÆ°á»›c 4: Láº¥y hidden state cuá»‘i cÃ¹ng =====
        final_embed = h_n.squeeze(0)                    # [26, 256]
        
        # ===== BÆ°á»›c 5: Dá»± bÃ¡o min/max =====
        out = self.linear(final_embed)                  # [26, 2]
        
        return out  # [26, 2]
</code></pre>
<p><strong>SÆ¡ Ä‘á»“ kiáº¿n trÃºc:</strong></p>
<pre><code>Input: 2 quÃ½ x 26 cá»• phiáº¿u x 4 features
  â†“
[QuÃ½ 1] â†’ [GAT Layer] â†’ [26, 256]
[QuÃ½ 2] â†’ [GAT Layer] â†’ [26, 256]
  â†“ Stack
[2 quÃ½, 26 nodes, 256 features]
  â†“
[LSTM Cell] â†’ Há»c pattern quÃ¡ thá»i gian
  â†“ Hidden state
[26, 256]
  â†“
[Linear Layer] â†’ Dá»± bÃ¡o
  â†“
Output: [26 cá»• phiáº¿u, 2 giÃ¡ trá»‹ (min, max)]
</code></pre>
<p><strong>Chi tiáº¿t tá»«ng layer:</strong></p>
<table>
<thead>
<tr>
<th>Layer</th>
<th>Input</th>
<th>Output</th>
<th>CÃ´ng dá»¥ng</th>
</tr>
</thead>
<tbody><tr>
<td>GAT</td>
<td>[26, 4]</td>
<td>[26, 256]</td>
<td>Há»c trá»ng sá»‘ cáº¡nh, aggregate hÃ ng xÃ³m</td>
</tr>
<tr>
<td>LSTM</td>
<td>[26, 2, 256]</td>
<td>[26, 256]</td>
<td>Há»c temporal pattern</td>
</tr>
<tr>
<td>Linear</td>
<td>[26, 256]</td>
<td>[26, 2]</td>
<td>Dá»± bÃ¡o min/max</td>
</tr>
</tbody></table>
<hr>
<h2>4ï¸âƒ£ Training Loop</h2>
<pre><code class="language-python">torch.manual_seed(42) 

model = StockGAT_LSTM(num_nodes=26, in_features=4, 
                      gnn_hidden=32, lstm_hidden=128, heads=2)

optimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-5)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=150)

split_idx = -4  # Chia dá»¯ liá»‡u: 4 quÃ½ cuá»‘i = test
train_seqs = sequences[:split_idx]
test_seqs = sequences[split_idx:]
</code></pre>
<h3>Training Loop</h3>
<pre><code class="language-python">best_loss, patience = float(&#39;inf&#39;), 0
loss_history = []
model.train()

for epoch in range(200):
    epoch_loss = 0.0
    
    for x_seq, y_target, sample_edge_index in train_seqs:
        # ===== Forward pass =====
        optimizer.zero_grad()                              # Reset gradient
        out = model(x_seq, sample_edge_index)              # [26, 2]
        
        # ===== TÃ­nh loss =====
        # Center loss: dá»± bÃ¡o mean(min, max) = (pred_min + pred_max)/2
        loss_c = criterion_center(out.mean(dim=1), y_target.mean(dim=1))
        
        # Range loss: dá»± bÃ¡o range = max - min
        pred_range = out[:, 1] - out[:, 0]
        targ_range = y_target[:, 1] - y_target[:, 0]
        
        loss_r = criterion_range(pred_range, targ_range)
        # Pháº¡t náº¿u dá»± bÃ¡o range quÃ¡ bÃ© (Range Compression)
        loss_r = loss_r + 2.0 * torch.relu(targ_range - pred_range).mean()
        # Pháº¡t náº¿u range Ã¢m
        loss_r = loss_r + 0.5 * torch.relu(-pred_range).mean()
        
        # Káº¿t há»£p 2 loss
        loss = alpha_c * loss_c + alpha_r * loss_r
        
        # ===== Backward pass =====
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.5)  # TrÃ¡nh gradient explosion
        optimizer.step()
        
        epoch_loss += loss.item()
    
    # ===== Epoch statistics =====
    avg_loss = epoch_loss / len(train_seqs)
    loss_history.append(avg_loss)
    scheduler.step()
    
    # ===== Early stopping =====
    if avg_loss &lt; best_loss - 1e-5:
        best_loss = avg_loss
        patience = 0
    else:
        patience += 1
    
    if (epoch + 1) % 25 == 0:
        print(f&quot;E{epoch+1:3d} | Loss: {avg_loss:.5f}&quot;)
    
    if patience &gt;= 15:
        print(f&quot;Early stop @ epoch {epoch+1}&quot;)
        break
</code></pre>
<hr>
<h3>1.Â <code>loss_mse</code>Â (Mean Squared Error - Äá»™ chÃ­nh xÃ¡c tuyá»‡t Ä‘á»‘i)</h3>
<ul>
<li><strong>Ã nghÄ©a toÃ¡n há»c:</strong>Â TÃ­nh trung bÃ¬nh bÃ¬nh phÆ°Æ¡ng sai sá»‘ giá»¯a giÃ¡ trá»‹ dá»± bÃ¡o vÃ  giÃ¡ trá»‹ thá»±c táº¿.</li>
<li><strong>Má»¥c Ä‘Ã­ch:</strong>Â Äáº£m báº£oÂ <strong>Ä‘á»™ chÃ­nh xÃ¡c vá» máº·t con sá»‘</strong>, Ã©p mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n ra cÃ¡c con sá»‘Â <code>Min Return</code>Â vÃ Â <code>Max Return</code>Â cÃ ng gáº§n vá»›i thá»±c táº¿ cÃ ng tá»‘t.</li>
</ul>
<h3>2.Â <code>loss_corr_min</code>Â (Pearson Correlation Loss - Báº¯t xu hÆ°á»›ng)</h3>
<ul>
<li><strong>Ã nghÄ©a:</strong>Â Há»‡ sá»‘ tÆ°Æ¡ng quan Pearson Ä‘o lÆ°á»ng má»©c Ä‘á»™ biáº¿n thiÃªn cÃ¹ng chiá»u cá»§a hai biáº¿n sá»‘ (tá»« -1 Ä‘áº¿n 1).<ul>
<li>Náº¿uÂ <code>pred</code>Â tÄƒng mÃ Â <code>true</code>Â cÅ©ng tÄƒngÂ â†’â†’Â Correlation caoÂ â†’â†’Â Loss tháº¥p.</li>
<li>NÃ³Â <strong>khÃ´ng quan tÃ¢m Ä‘áº¿n Ä‘á»™ lá»›n</strong>Â (scale) mÃ  chá»‰ quan tÃ¢m Ä‘áº¿nÂ <strong>hÃ¬nh dÃ¡ng Ä‘á»“ thá»‹</strong>Â (shape).</li>
</ul>
</li>
<li><strong>Táº¡i sao chá»‰ dÃ¹ng choÂ <code>Min Return</code>?</strong>Â Trong Ä‘oáº¡n code, báº¡n viáº¿tÂ <code>correlation_loss(pred_min, true_min)</code>. Äiá»u nÃ y Ã¡m chá»‰ báº¡n Ä‘áº·c biá»‡t quan tÃ¢m Ä‘áº¿n viá»‡cÂ <strong>báº¯t Ä‘Ãºng xu hÆ°á»›ng rá»§i ro</strong>Â (khi nÃ o thá»‹ trÆ°á»ng sáº­p, khi nÃ o thá»‹ trÆ°á»ng Ä‘Ã¡y) hÆ¡n lÃ  xu hÆ°á»›ng lá»£i nhuáº­n tá»‘i Ä‘a.</li>
<li><strong>Má»¥c Ä‘Ã­ch:</strong>Â GiÃºp mÃ´ hÃ¬nh há»c Ä‘Æ°á»£c &quot;Trend&quot;. VÃ­ dá»¥: DÃ¹ dá»± bÃ¡o sai sá»‘ liá»‡u (lá»‡ch 5%), nhÆ°ng náº¿u thá»‹ trÆ°á»ng giáº£m mÃ  mÃ´ hÃ¬nh cÅ©ng bÃ¡o giáº£m thÃ¬ váº«n Ä‘Æ°á»£c coi lÃ  tá»‘t.</li>
</ul>
<h3>3.Â <code>risk_penalty</code>Â (Safety Loss - Pháº¡t sá»± láº¡c quan thÃ¡i quÃ¡)</h3>
<ul>
<li><strong>CÆ¡ cháº¿ hoáº¡t Ä‘á»™ng:</strong><ul>
<li><code>pred_min</code>Â lÃ  má»©c sá»¥t giáº£m dá»± bÃ¡o (vÃ­ dá»¥: -5%).</li>
<li><code>true_min</code>Â lÃ  má»©c sá»¥t giáº£m thá»±c táº¿ (vÃ­ dá»¥: -10%).</li>
<li>Náº¿uÂ <code>pred_min &gt; true_min</code>Â (Dá»± bÃ¡o -5% nhÆ°ng thá»±c táº¿ sáº­p -10%): Báº¡n Ä‘Ã£Â <strong>quÃ¡ láº¡c quan</strong>, Ä‘Ã¡nh giÃ¡ tháº¥p rá»§i roÂ â†’â†’Â <code>pred - true &gt; 0</code>Â â†’â†’Â <code>ReLU</code>Â giá»¯ láº¡i giÃ¡ trá»‹ nÃ yÂ â†’â†’Â <strong>Bá»‹ pháº¡t (Loss tÄƒng)</strong>.</li>
<li>Náº¿uÂ <code>pred_min &lt; true_min</code>Â (Dá»± bÃ¡o -15% nhÆ°ng thá»±c táº¿ chá»‰ giáº£m -10%): Báº¡nÂ <strong>tháº­n trá»ng/bi quan</strong>Â â†’â†’Â <code>pred - true &lt; 0</code>Â â†’â†’Â <code>ReLU</code>Â biáº¿n thÃ nh 0Â â†’â†’Â <strong>KhÃ´ng bá»‹ pháº¡t</strong>.</li>
</ul>
</li>
<li><strong>Má»¥c Ä‘Ã­ch:</strong>Â ÄÃ¢y lÃ  cÆ¡ cháº¿Â <strong>An toÃ n (Safety)</strong>. Trong Ä‘áº§u tÆ°, viá»‡c Ä‘Ã¡nh giÃ¡ tháº¥p rá»§i ro (Underestimating Risk) nguy hiá»ƒm hÆ¡n nhiá»u so vá»›i viá»‡c Ä‘Ã¡nh giÃ¡ quÃ¡ cao rá»§i ro. HÃ m nÃ y Ã©p mÃ´ hÃ¬nh thÃ  dá»± bÃ¡o &quot;xáº¥u hÆ¡n thá»±c táº¿&quot; cÃ²n hÆ¡n lÃ  &quot;tá»‘t hÆ¡n thá»±c táº¿&quot;.</li>
</ul>
<h3>4. Tá»•ng há»£p Loss (Weighted Sum)</h3>
<ul>
<li><strong>50% (MSE):</strong>Â Váº«n Æ°u tiÃªn Ä‘á»™ chÃ­nh xÃ¡c cá»§a con sá»‘ nháº¥t.</li>
<li><strong>45% (Correlation):</strong>Â Ráº¥t coi trá»ng viá»‡c báº¯t Ä‘Ãºng Trend (xu hÆ°á»›ng tÄƒng/giáº£m). ÄÃ¢y lÃ  tá»· trá»ng ráº¥t cao, biáº¿n mÃ´ hÃ¬nh thÃ nh dáº¡ngÂ <strong>Trend-Following</strong>.</li>
<li><strong>5% (Risk Penalty):</strong>Â Má»™t chÃºt gia vá»‹ Ä‘á»ƒ nháº¯c nhá»Ÿ mÃ´ hÃ¬nh &quot;hÃ£y tháº­n trá»ng, Ä‘á»«ng láº¡c quan quÃ¡&quot;.</li>
</ul>
<hr>
<h2>Evaluation &amp; Visualization</h2>
<h3>Cell 13: ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh</h3>
<pre><code class="language-python">model.eval()
preds, targets = [], []

with torch.no_grad():
    for x_seq, y_target, sample_edge_index in test_seqs:
        out = model(x_seq, sample_edge_index)
        preds.append(out.numpy())
        targets.append(y_target.numpy())

# ===== Metric 1: Range Compression =====
pred_ranges = []
true_ranges = []

for pred, target in zip(preds, targets):
    pred_range = (pred[:, 1] - pred[:, 0]).mean()  # Average range
    true_range = (target[:, 1] - target[:, 0]).mean()
    
    pred_ranges.append(pred_range)
    true_ranges.append(true_range)

range_ratio = (np.array(pred_ranges) / (np.array(true_ranges) + 1e-8)).mean()
range_bias = (np.array(pred_ranges) - np.array(true_ranges)).mean()

print(f&quot;\nğŸ“Š Range Statistics:&quot;)
print(f&quot;   True Range (mean)  : {np.array(true_ranges).mean():.6f}&quot;)
print(f&quot;   Pred Range (mean)  : {np.array(pred_ranges).mean():.6f}&quot;)
print(f&quot;   Range Ratio        : {range_ratio:.4f} {&#39;âŒ TOO NARROW&#39; if range_ratio &lt; 0.95 else &#39;âœ… OK&#39;}&quot;)
print(f&quot;   Range Bias         : {range_bias:.6f}&quot;)

# ===== Metric 2: Direction Accuracy =====
pred_dir_max = np.sign(np.array([p[:, 1] for p in preds]))
true_dir_max = np.sign(np.array([t[:, 1] for t in targets]))

direction_accuracy = (pred_dir_max == true_dir_max).mean() * 100
print(f&quot;\nğŸ“ˆ Direction Accuracy (Max): {direction_accuracy:.2f}%&quot;)

# ===== Metric 3: Error Metrics =====
err_max = np.array([p[:, 1] for p in preds]) - np.array([t[:, 1] for t in targets])
mae_max = np.abs(err_max).mean()
mse_max = (err_max ** 2).mean()
rmse_max = np.sqrt(mse_max)

print(f&quot;\nâŒ Error Metrics (Max):&quot;)
print(f&quot;   MAE  : {mae_max:.6f}&quot;)
print(f&quot;   RMSE : {rmse_max:.6f}&quot;)
</code></pre>
<p><strong>CÃ¡c metric Ä‘Ã¡nh giÃ¡:</strong></p>
<table>
<thead>
<tr>
<th>Metric</th>
<th>CÃ´ng thá»©c</th>
<th>Ã nghÄ©a</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Range Ratio</strong></td>
<td>pred_range / true_range</td>
<td>Náº¿u = 1.0: perfect. &lt; 1.0: under-predict. &gt; 1.0: over-predict</td>
</tr>
<tr>
<td><strong>Direction Accuracy</strong></td>
<td>% láº§n dá»± bÃ¡o Ä‘Ãºng chiá»u (â†‘ vs â†“)</td>
<td>DÃ¹ sai giÃ¡, dá»± bÃ¡o Ä‘Ãºng hÆ°á»›ng cÅ©ng tá»‘t</td>
</tr>
<tr>
<td><strong>MAE</strong></td>
<td>Mean Absolute Error</td>
<td>Sai lá»‡ch trung bÃ¬nh tuyá»‡t Ä‘á»‘i</td>
</tr>
<tr>
<td><strong>RMSE</strong></td>
<td>Root Mean Squared Error</td>
<td>Nháº¥n máº¡nh sai sá»‘ lá»›n hÆ¡n MAE</td>
</tr>
</tbody></table>
\n    </article>\n    <footer style="margin-top: 4rem; padding-top: 2rem; border-top: 2px solid #d3c6a6; text-align: center; color: #927e5a;">\n        <p>Â© 2026 My Blog</p>\n    </footer>\n</body>\n</html>